#!/usr/bin/env python3
"""
Simple AMP test for PatchedBrainTransformer
"""

import torch

def test_amp_simple():
    """Simple AMP test"""
    print("üß™ Testing AMP for PatchedBrainTransformer")
    print("=" * 45)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available - cannot test AMP")
        return False
    
    print(f"‚úÖ PyTorch Version: {torch.__version__}")
    print(f"‚úÖ CUDA Version: {torch.version.cuda}")
    
    # Try importing AMP
    try:
        from torch.cuda.amp import autocast, GradScaler
        print("‚úÖ AMP Import: SUCCESS (torch.cuda.amp)")
        
        # Test basic AMP functionality
        device = torch.device('cuda')
        
        # Create a simple model
        model = torch.nn.Linear(10, 1).to(device)
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        scaler = GradScaler()
        
        # Test data
        x = torch.randn(5, 10, device=device)
        target = torch.randn(5, 1, device=device)
        
        # Forward pass with autocast
        with autocast():
            output = model(x)
            loss = torch.nn.functional.mse_loss(output, target)
        
        # Backward pass with scaler
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        print("‚úÖ AMP Test: SUCCESS")
        print(f"   Loss: {loss.item():.4f}")
        print("   üöÄ AMP is working correctly!")
        
        return True
        
    except Exception as e:
        print(f"‚ùå AMP Test: FAILED - {e}")
        return False

def test_training_imports():
    """Test if training imports work"""
    print("\nüîç Testing Training Imports")
    print("=" * 30)
    
    try:
        from src.train import training
        print("‚úÖ Training import: SUCCESS")
        
        # Test if AMP variables are set correctly
        import src.train as train_module
        if hasattr(train_module, 'AMP_AVAILABLE'):
            print(f"‚úÖ AMP_AVAILABLE: {train_module.AMP_AVAILABLE}")
        else:
            print("‚ö†Ô∏è AMP_AVAILABLE not found")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Training import: FAILED - {e}")
        return False

def main():
    """Main test function"""
    print("üß† PatchedBrainTransformer - AMP Test")
    print("=" * 50)
    
    amp_works = test_amp_simple()
    imports_work = test_training_imports()
    
    print(f"\nüìã Test Summary:")
    print(f"   AMP Functionality: {'‚úÖ' if amp_works else '‚ùå'}")
    print(f"   Training Imports: {'‚úÖ' if imports_work else '‚ùå'}")
    
    if amp_works and imports_work:
        print("\nüéâ All tests passed! Ready for training with AMP.")
    elif imports_work:
        print("\n‚ö†Ô∏è Training will work but without AMP acceleration.")
    else:
        print("\n‚ùå Issues detected. Check PyTorch installation.")

if __name__ == "__main__":
    main()
